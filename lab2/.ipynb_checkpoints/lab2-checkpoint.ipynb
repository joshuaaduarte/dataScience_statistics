{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c97d7e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1><center>SDSE Lab 2 <br><br> Gradient descent and <br> Stochastic Gradient Descent </center></h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30150b0b",
   "metadata": {},
   "source": [
    "Note: The format for the report is as a Jupyter Notebook. Please include the SIDs of the members of your group in the `results` dictionary. A single notebook should be submitted as a group submission in Gradescope. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {\n",
    "    'SIDs': [0,0] # enter the SIDs for the group members\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fcc3e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this lab we will explore the gradient descent and stochastic gradient descent algorithms for solving a least squares optimization problem. The setup is as follows. We wish to model a process with scalar input $X$ and scalar output $Y$. Both of these are real-valued random variables; their sample spaces are the real line. The joint distribution of $X$ and $Y$ is given as:\n",
    "\\begin{align*}\n",
    "X &\\sim \\mathcal{U}(0,1) \\\\\n",
    "Y|X\\!=\\!x &\\sim \\mathcal{N}( \\theta_0 + \\theta_1 x ,\\sigma_\\epsilon)\n",
    "\\end{align*}\n",
    "This definition of $Y|X\\!=\\!x$ is equivalent to,\n",
    "\\begin{equation*}\n",
    "Y = \\theta_0 + \\theta_1 X + \\epsilon\n",
    "\\end{equation*}\n",
    "with $\\epsilon\\sim\\mathcal{N}(0,\\sigma_\\epsilon^2)$.\n",
    "\n",
    "# 1. Sampling the joint distribution\n",
    "\n",
    "We will first construct a synthetic dataset by sampling  from $(X,Y)$. \n",
    "\n",
    "Write a function called `sampleXY` that produces a dataset $\\{(x_i,y_i)\\}_N$ of iid samples from $(X,Y)$, given arguments $N$, $\\theta_0$, $\\theta_1$, and $\\sigma_\\epsilon$. The output of this function should be a numpy array with shape = $(N,2)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed7c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleXY(N, theta0, theta1, sigma_eps):\n",
    "    pass   # ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32611c5a",
   "metadata": {},
   "source": [
    "Run `sampleXY` with $N\\!=\\!40$, $\\theta_0\\!=\\!0.2$, $\\theta_1\\!=\\!-0.4$, and $\\sigma_\\epsilon\\!=\\!0.07$ and assign the result to the variable `XYsamp`. Create a plot showing the line $y=\\theta_0 + \\theta_1 x$, overlaid with a scatter plot of `XYsamp`. The plot should have labels on the x and y axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ecf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 40\n",
    "theta0=0.2\n",
    "theta1=-0.4\n",
    "sigma_eps= 0.07\n",
    "XYsamp = sampleXY(N, theta0, theta1, sigma_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af952ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "\n",
    "# ADD CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47574322",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Reporting. Do not modify.#####\n",
    "result['fig1'] = fig1  # 4 points\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb8f32",
   "metadata": {},
   "source": [
    "We will now overwrite the data you sampled with another dataset contained in the file `1d_data.pickle`. This is so the results are predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a726b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./1d_data.pickle', 'rb') as file:\n",
    "    XYsamp = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b80c9e",
   "metadata": {},
   "source": [
    "# 2. Gradient descent for linear regression\n",
    "\n",
    "We are still a few weeks from learning about linear regression. However here we will use the optimization problem at the center of linear regression to demonstrate gradient descent and stochastic gradient descent. \n",
    "\n",
    "Our dataset `XYsamp` contains $N$ tuples $(x_i,y_i)$. Our goal now is to find parameters $\\hat\\theta_0$ and $\\hat\\theta_1$ that minimize a quadratic cost function. \n",
    "\n",
    "\\begin{equation*}\n",
    "J(\\hat\\theta_0,\\hat\\theta_1) = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\hat\\theta_0 + \\hat \\theta_1 x_i - y_i \\right)^2\n",
    "\\end{equation*}\n",
    "\n",
    "The update equation for gradient descent is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\theta_{k+1} = \\theta_k -\\gamma \\nabla J(\\theta_k)\n",
    "\\end{equation*}\n",
    "\n",
    "$\\theta_{k}$ is a vector of the estimates $\\hat\\theta_0$ and $\\hat\\theta_1$ after $k$ steps of gradient descent. The gradient of $J$ is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla J(\\hat\\theta_0,\\hat\\theta_1) &= \n",
    "\\frac{1}{N} \\sum_{i=1}^{N} \\nabla\\left( \\hat\\theta_0 + \\hat \\theta_1 x_i - y_i \\right)^2  \\\\\n",
    "&= \n",
    "\\frac{1}{N} \\sum_{i=1}^{N} 2\\left( \\hat\\theta_0 + \\hat \\theta_1 x_i - y_i \\right)\\begin{bmatrix} 1 \\\\ x_i \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "## (2a)\n",
    "\n",
    "Write a function called `nablaJ` that takes `XYsamp`, $\\hat\\theta_0$, and $\\hat\\theta_1$ as inputs, and returns the gradient as a numpy array of length 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72128258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nablaJ(XYsamp, theta0, theta1):\n",
    "    pass   # ADD CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa66f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Reporting. Do not modify.#####\n",
    "result['nablaJ_2a_a'] = nablaJ(XYsamp,0.5,-1)    # 3 points\n",
    "result['nablaJ_2a_b'] = nablaJ(XYsamp,1.1,0.6)    # 3 points\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4ed8f",
   "metadata": {},
   "source": [
    "## (2b)\n",
    "\n",
    "Write a function called `gradient_descent` that executes the gradient descent algorithm. This function should take as input \n",
    "\n",
    "+ The dataset `XYsamp`\n",
    "+ the total number of steps to take `K`\n",
    "+ the step size `gamma`\n",
    "+ the initial condition `theta_init` as a numpy array of length 2. \n",
    "\n",
    "It should return the trajectory as a 2D numpy array with shape = `(K,2)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(XYsamp,K,gamma,theta_init):\n",
    "\n",
    "    d = XYsamp.shape[1]\n",
    "    Theta = np.empty((K, d))\n",
    "    \n",
    "    # ADD CODE HERE\n",
    "    \n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54309f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Reporting. Do not modify.#####\n",
    "result['gd_2b_a'] = gradient_descent(XYsamp,10,0.1,np.array([-0.5,0.5]))  # 4 points\n",
    "result['gd_2b_b'] = gradient_descent(XYsamp,20,0.01,np.array([0.5,-0.5]))  # 4 points\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7384232",
   "metadata": {},
   "source": [
    "## (2c)\n",
    "\n",
    "Complete the `run_gd_on_grid(theta0_grid,theta1_grid,K,gamma)`. This function takes as input\n",
    "+ `theta0_grid` and `theta1_grid`. These are two 5x5 grids of values of $\\hat\\theta_0$ and $\\hat\\theta_1$. \n",
    "+ the total number of steps to take `K`\n",
    "+ the step size `gamma`\n",
    "\n",
    "The function should return a numpy array with shape (5,5,K,2), where the (i,j,:,:) is a (K,2) trajectory of parameter values, and (i,j) are the indices to the 5x5 arrays.\n",
    "\n",
    "Run the function with $K=200$, $\\gamma=0.2$ and save the result to `trajectories`.\n",
    "\n",
    "Note: The code for creating the 5x5 grid is provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5944e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not alter this code. It creates the 5x5 grid of values \n",
    "# that are passed to `run_gd_on_grid`\n",
    "def make_grid(gridN):\n",
    "    theta_0_array = np.linspace(-1,1,gridN)\n",
    "    theta_1_array = np.linspace(-1,1,gridN)\n",
    "    return  np.meshgrid(theta_0_array,theta_1_array)\n",
    "\n",
    "gridN = 5\n",
    "theta0_grid,theta1_grid = make_grid(gridN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af14c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gd_on_grid(theta0_grid,theta1_grid,XYsamp,K,gamma):\n",
    "    gridN = theta0_grid.shape[0]\n",
    "    traj = np.empty((gridN,gridN,K,2))\n",
    "    \n",
    "    # ADD CODE HERE\n",
    "    \n",
    "    return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f10e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = run_gd_on_grid(theta0_grid,theta1_grid,XYsamp,K=200,gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398df9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Reporting. Do not modify.#####\n",
    "result['grid_2c'] = run_gd_on_grid(theta0_grid,theta1_grid,XYsamp,K=200,gamma=0.2) #  8 points\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88585ae",
   "metadata": {},
   "source": [
    "## (2d)\n",
    "\n",
    "In this synthetic example, we actually know the true values of $\\theta_0$ and $\\theta_1$, since these were used to generate the data (they are stored as `theta0` and `theta1`. So we can explicitly compute the 2-norm of the estimation error for candidates $\\hat\\theta_0$ and $\\hat\\theta_1$ with:\n",
    "\n",
    "\\begin{equation*}\n",
    "e = \\sqrt{ (\\hat\\theta_0-\\theta_0)^2 + (\\hat\\theta_1-\\theta_1)^2 } \n",
    "\\end{equation*}\n",
    "\n",
    "Write a function called `compute_error` that makes this calculation. The function should take inputs `theta0hat`, `theta1hat`, `theta0`, `theta1`, and should return a scalar value for the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(theta0hat, theta1hat, theta0, theta1):\n",
    "    pass # ADD CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4e7a5",
   "metadata": {},
   "source": [
    "The following method will use your `compute_error` method to plot the evolution of the error in gradient descent. It plots the error trajectory for all 25 initial conditions on a 5x5 grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f374a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(trajectories):\n",
    "    \n",
    "    gridN = trajectories.shape[0]\n",
    "    K = trajectories.shape[2]\n",
    "    d = trajectories.shape[3]\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    for i0 in range(gridN):\n",
    "        for i1 in range(gridN):\n",
    "            traj = trajectories[i0,i1,:,:]\n",
    "            error = np.empty(K)\n",
    "            for k in range(K):\n",
    "                error[k] = compute_error(traj[k,0], traj[k,1], theta0, theta1)\n",
    "            plt.semilogy(range(K),error)\n",
    "\n",
    "    plt.xlabel('k',fontsize=15)\n",
    "    plt.ylabel('error',fontsize=15)\n",
    "    plt.grid()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77adc8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Reporting. Do not modify.#####\n",
    "result['fig_2d'] = plot_error(result['grid_2c']) # 4 points\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c71b7",
   "metadata": {},
   "source": [
    "## (2e)\n",
    "\n",
    "The `plot_quiver` function provided below creates a vector field plot of the negative gadient $-\\nabla J(\\theta_0,\\theta_1)$. It also marks the true value of the parameters with a blue dot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392df49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quiver(XYsamp):    \n",
    "    gridN = 10\n",
    "    theta0_grid, theta1_grid = make_grid(gridN)\n",
    "    flatgrid = np.reshape([theta0_grid, theta1_grid],(2,gridN**2)).T\n",
    "    UV = np.empty(flatgrid.shape)\n",
    "    for i, (theta0z, theta1z) in enumerate(flatgrid):\n",
    "        UV[i,:] = nablaJ(XYsamp,theta0z, theta1z)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    plt.quiver(flatgrid[:,0], flatgrid[:,1],-UV[:,0],-UV[:,1],scale=N)\n",
    "    \n",
    "    plt.xlabel('theta0',fontsize=15)\n",
    "    plt.ylabel('theta1',fontsize=15)\n",
    "    plt.plot(theta0,theta1,'o',markersize=16)\n",
    "    plt.axis([-1,1,-1,1])\n",
    "    return fig\n",
    "\n",
    "plot_quiver(XYsamp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0254f99f",
   "metadata": {},
   "source": [
    "\n",
    "Your task is to complete the `plot_traj` function. The inputs to this function are:\n",
    "+ `fig`: the output of `plot_quiver` (a figure handle)\n",
    "+ `trajectories`: a set of gradient descent trajectories generate by `run_gd_on_grid`.\n",
    "\n",
    "The function should overlay these trajectories on top of the vector field. The style of the trajectory lines should be `'.-'` and it should have these characteristics:\n",
    "+ `color='red'`\n",
    "+ `linewidth=0.5`\n",
    "+ `markersize=2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d070a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traj(fig,trajectories):  \n",
    "    \n",
    "    plt.figure(fig)\n",
    "       \n",
    "    # ADD CODE HERE\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be316da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_traj(plot_quiver(XYsamp),trajectories);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4752cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Reporting. Do not modify.#####\n",
    "result['fig_2e'] = plot_traj(plot_quiver(XYsamp),result['grid_2c'])  # 6 points\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa31eed",
   "metadata": {},
   "source": [
    "## (2f)\n",
    "\n",
    "The next two cells generate the convergence and phase plots with $\\gamma=0.01$ and $\\gamma=0.7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35acbbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories2 = run_gd_on_grid(theta0_grid,theta1_grid,XYsamp,K=200,gamma=0.01)\n",
    "plot_traj(plot_quiver(XYsamp),trajectories2)\n",
    "plot_error(trajectories2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69df8598",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories3 = run_gd_on_grid(theta0_grid,theta1_grid,XYsamp,K=200,gamma=0.7)\n",
    "plot_traj(plot_quiver(XYsamp),trajectories3)\n",
    "plot_error(trajectories3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079804df",
   "metadata": {},
   "source": [
    "What do you observe about the influence of the step size on the convergence of gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eb3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ffbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Reporting. Do not modify.#####\n",
    "result['comment2f'] = comment   # 4 points\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f24ab1",
   "metadata": {},
   "source": [
    "# 3. Additive cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402e8fe",
   "metadata": {},
   "source": [
    "There is no deliverable for this part. The goal is to introduce a plot that will be used in the next part. Run the next cell to generate the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersections(XYsamp):\n",
    "    x = XYsamp[:,0]\n",
    "    y = XYsamp[:,1]\n",
    "    N = XYsamp.shape[0]\n",
    "    points = []\n",
    "    for a in range(N - 1):\n",
    "        for b in np.arange(a + 1, N):\n",
    "            p0 = (y[a] * x[b] - y[b] * x[a]) / (x[b] - x[a])\n",
    "            p1 = (y[b] - y[a]) / (x[b] - x[a])\n",
    "            points.append([p0, p1])\n",
    "    return np.array(points)\n",
    "\n",
    "def plot_additive_grads(XYsamp):\n",
    "\n",
    "    x = XYsamp[:,0]\n",
    "    y = XYsamp[:,1]\n",
    "    \n",
    "    points = find_intersections(XYsamp)\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    for n in range(N):\n",
    "        plt.plot([-1,(1+y[n])/x[n]],[y[n]+x[n],-1],'k',linewidth=0.2)\n",
    "    plt.plot(points[:,1],points[:,0],'k.',markersize=3)\n",
    "#     plt.plot(theta1,theta0,'mo',markersize=8,markerfacecolor='orange')\n",
    "    plt.xlabel('theta1',fontsize=15)\n",
    "    plt.ylabel('theta0',fontsize=15)\n",
    "    plt.axis([-1,1,-1,1])\n",
    "    return fig\n",
    "\n",
    "plot_additive_grads(XYsamp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6469b",
   "metadata": {},
   "source": [
    "In the context of stochastic gradient descent, the cost function that we are trying to minimize is the average over $N$ small cost functions:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ell_i(\\hat\\theta_0,\\hat\\theta_1) = \\left( \\hat\\theta_0 + \\hat \\theta_1 x_i - y_i \\right)^2\n",
    "\\end{equation*}\n",
    "\n",
    "each one associated with the data point $(x_i,y_i)$.\n",
    "\n",
    "\\begin{align*}\n",
    "J(\\hat\\theta_0,\\hat\\theta_1) &= \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\hat\\theta_0 + \\hat \\theta_1 x_i - y_i \\right)^2 \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^{N} \\ell_i(\\hat\\theta_0,\\hat\\theta_1)\n",
    "\\end{align*}\n",
    "\n",
    "The gradient of $J$ is then the average if the gradients of the $\\ell_i$'s:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla J(\\hat\\theta_0,\\hat\\theta_1) &= \\frac{1}{N} \\sum_{i=1}^{N} \\nabla\\ell_i(\\hat\\theta_0,\\hat\\theta_1)\n",
    "\\end{align*}\n",
    "\n",
    "Let's take a particular $\\ell_i$ and find its minimum. Since $\\ell_i$ is always positive, its minimum is wherever it equals zero. This is along a line in the $(\\hat\\theta_0,\\hat\\theta_1)$ plane:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat\\theta_0 + \\hat\\theta_1 x_i - y_i = 0\n",
    "\\end{equation*}\n",
    "\n",
    "The plot above shows each of those $N$ lines. The black dots are the intersections of two lines. These would be exact solutions if only a pair of data points were given ($N=2$). \n",
    "\n",
    "# 4. Stochastic Gradient Descent\n",
    "\n",
    "## 4(a)\n",
    "\n",
    "Code stochastic gradient descent. Complete the function `SGD` below. This function takes as arguments \n",
    "+ the dataset `XYsamp`, \n",
    "+ the step size $\\gamma$ and \n",
    "+ the number of epochs to run. \n",
    "\n",
    "`SGD` function should\n",
    "\n",
    "+ randomly choose the initial condition with uniform probability from $[-1,1]\\times[-1,1]$\n",
    "+ use batches of size 1,\n",
    "+ draw samples without replacement.\n",
    "\n",
    "The function should return the parameter trajectory. \n",
    "\n",
    "\n",
    "Run SGD with $\\gamma=0.1$ and 10 epochs. Recreate the plot from part 2 but using this SGD trajectory instead of GD. Save the figure handle as `fig4a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(XYsamp,gamma,epochs):\n",
    "    \n",
    "    x = XYsamp[:,0]\n",
    "    y = XYsamp[:,1]\n",
    "    N = XYsamp.shape[0]\n",
    "    traj = np.empty((epochs*N+1,2))\n",
    "    \n",
    "    # ADD YOUR CODE HERE\n",
    "            \n",
    "    sol = traj[:-1,:]\n",
    "            \n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895928a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4a = plot_additive_grads(XYsamp)\n",
    "\n",
    "traj = SGD(XYsamp,0.1,10)\n",
    "plt.plot(traj[:,1],traj[:,0],'r.-',linewidth=0.5,markersize=2)\n",
    "plt.plot(traj[0,1],traj[0,0],'og')\n",
    "plt.plot(traj[-1,1],traj[-1,0],'or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e89c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Reporting. Do not modify.#####\n",
    "result['fig4a'] = fig4a # 8 points\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a725139",
   "metadata": {},
   "source": [
    "## 4(b)\n",
    "\n",
    "Make the same plot with $\\gamma=0.01$ and $\\gamma=0.4$. Save the figure handles respectively as `fig4b1` and `fig4b2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4b1 = plot_additive_grads(XYsamp)\n",
    "\n",
    "traj = SGD(XYsamp,0.01,10)\n",
    "plt.plot(traj[:,1],traj[:,0],'r.-',linewidth=0.5,markersize=2)\n",
    "plt.plot(traj[0,1],traj[0,0],'og')\n",
    "plt.plot(traj[-1,1],traj[-1,0],'or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf3634",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4b2 = plot_additive_grads(XYsamp)\n",
    "\n",
    "traj = SGD(XYsamp,0.4,10)\n",
    "plt.plot(traj[:,1],traj[:,0],'r.-',linewidth=0.5,markersize=2)\n",
    "plt.plot(traj[0,1],traj[0,0],'og')\n",
    "plt.plot(traj[-1,1],traj[-1,0],'or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ac818",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Reporting. Do not modify.#####\n",
    "result['fig4b1'] = fig4b1   # 4 points\n",
    "result['fig4b2'] = fig4b2   # 4 points\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90721049",
   "metadata": {},
   "source": [
    "## 4(c)\n",
    "\n",
    "Comment on the pros and cons of SGD with respect to GD. Save your comments in a string called `comment`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f907248",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8fb494",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# Reporting. Do not modify.#####\n",
    "result['comment'] = comment   # 4 points\n",
    "################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d80fb9",
   "metadata": {},
   "source": [
    "---\n",
    "## Do not modify below this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}.pickle'.format(\"_\".join([str(sid) for sid in result['SIDs']])),'wb') as file:\n",
    "    pickle.dump(result,file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
